import os
from gensim import models
import json
import numpy as np
from keras.models import *
from keras.layers import *
import keras
from sklearn.metrics import *
import keras.backend as K
import pandas as pd

def readJson(filename):
	print "Reading [%s]..." % (filename)
	with open(filename) as inputFile:
		jsonData = json.load(inputFile)
	print "Finished reading [%s]." % (filename)
	return jsonData

def extractFeatures(questionRows, totalLength, maxLength):
	questionKeys = questionRows.keys()

	print 'Total Question Rows: [%d]' % (len(questionRows))
	print 'Filtered Question Rows: [%d]' % (len(questionKeys))

	X = []
	y = []

	print 'Max Sequence Length: [%d]' % (maxLength)
	print 'Total Sequence Length: [%d]' % (totalLength)

	for i,questionRowIndex in enumerate(questionKeys):
		questionRow = questionRows[questionRowIndex]
		imageFilename = questionRow['image']
		caption = imageCaptions[imageFilename]

		labels = [int(l) for l in questionRow['wordLabels'].split(' ')]
		questionWords = questionRow['question'].split(' ')
		captionWords = caption.split(' ')

		newLabels = []
		newQuestionWords = []
		newCaptionWords = []
		for wi, w in enumerate(questionWords):
			if w in w2v:
				if w.lower() not in excludeWordList:
					newQuestionWords.append(w)
					newLabels.append(labels[wi])
		for w in captionWords:
			if w in w2v:
				if w.lower() not in excludeWordList:
					newCaptionWords.append(w)

		labels = newLabels
		questionWords = newQuestionWords
		captionWords = newCaptionWords

		captionFeature = np.zeros((maxLength, wordVectorSize))
		questionFeature = np.zeros((maxLength, wordVectorSize))
		labelFeature = np.zeros((totalLength,2))
		# labelFeature = np.ones((totalLength,1))

		for li in range(0,maxLength):
			labelFeature[li][1] = 1

		for li,l in enumerate(labels):
			labelFeature[maxLength+li][l] = 1
			# labelFeature[li][l] = 1

		for ci,c in enumerate(captionWords):
			captionFeature[ci] = w2v[c]

		for ci,c in enumerate(questionWords):
			questionFeature[ci] = w2v[c]

		X.append(np.append(captionFeature,questionFeature, axis=0))
		# X.append(questionFeature)
		y.append(labelFeature)

	return np.asarray(X),np.asarray(y)


word2VecPath = '/sb-personal/cvqa/data/word2vec/google-news/GoogleNews-vectors-negative300.bin'
captionFile = '/sb-personal/cvqa/data/cvqa/imagecaptions.json'
trainFile = '/sb-personal/cvqa/data/cvqa/cvqa-sameQuestionDataset-subset-list3-train.json'
testFile = '/sb-personal/cvqa/data/cvqa/cvqa-sameQuestionDataset-subset-list3-test.json'

maxQuestionLength = 8
maxCaptionLength = 16
wordVectorSize = 300
embeddingSize = 250
numberOfEpochs = 60
subsetCount = 4000
maxLength = 20
totalLength = maxLength * 2
# totalLength = maxLength
n_hidden = 40
excludeWordList = ['is','a','the','what','that','to','who','why']

print "Loading Word2Vec Dictionary. This may take a long time..."
w2v = models.Word2Vec.load_word2vec_format(word2VecPath, binary=True)

print "Loading Captions generated by a Pre-Trained Captioning Model for Images..."
imageCaptions = readJson(captionFile)

print "Loading Questions..."

trainRows = readJson(trainFile)
testRows = readJson(testFile)

X_train, y_train = extractFeatures(trainRows, totalLength, maxLength)
X_test, y_test = extractFeatures(testRows, totalLength, maxLength)

print 'Total data samples: [%d]' % (len(y_train) + len(y_test))
print '\tTraining data size: [%d]' % (len(y_train))
print '\tTest data size: [%d]' % (len(y_test))

decoder = Sequential()
decoder.add(GRU(n_hidden, return_sequences=True, input_shape=(totalLength, wordVectorSize)))
decoder.add(TimeDistributed(Dense(n_hidden/2)))
decoder.add(TimeDistributed(Dense(2, activation='softmax')))
decoder.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy', 'precision','recall'])

print(decoder.summary())
print(decoder.get_config())

decoder.fit(X_train,y_train, nb_epoch=numberOfEpochs, verbose=1)

print decoder.test_on_batch(X_test, y_test)
# y_predict = decoder.predict_proba(X_test, verbose=0)
# # print 'Before'
# # print y_predict
# for value in np.nditer(y_predict, op_flags=['readwrite']):
# 	if value > 0.15:
# 		value[...] = 1
# 	else:
# 		value[...] = 0
# # print 'After'
# # print y_predict

# y_predict = np.reshape(y_predict,(len(y_test),totalLength))
# y_test = np.reshape(y_test,(len(y_test),totalLength))
# # print 'Recall: [%f]' % (recall_score(y_test,y_predict))
# # print 'Precision: [%f]' % (precision_score(y_test,y_predict))
# print 'Accuracy: [%f]' % (accuracy_score(y_test,y_predict))

# totals = []
# for p,_ in enumerate(y_predict):
# 	totals.append({'predict':np.array_str(y_predict[p]), 'actual':np.array_str(y_test[p])})

# outputFile = '/sb-personal/cvqa/data/cvqa/output.csv'
# pd.DataFrame(totals).to_csv(outputFile)