import os
from gensim import models
import json
import numpy as np
from keras.models import *
from keras.layers import *
import keras
from sklearn.metrics import *
import keras.backend as K
import pandas as pd

def readJson(filename):
	print "Reading [%s]..." % (filename)
	with open(filename) as inputFile:
		jsonData = json.load(inputFile)
	print "Finished reading [%s]." % (filename)
	return jsonData

def score(groundTruth, prediction, label):
	thresh=0.15
	pred=prediction>thresh
	gt=np.asarray(groundTruth)>0.5
	pred_n=prediction<thresh
	gt_n=np.asarray(groundTruth)<0.5

	recall_pos = recall_score(gt,pred)
	recall_neg = recall_score(gt_n,pred_n)
	print "\t%s - normalized accuracy: \t%f " % (label, (recall_score(gt,pred)+recall_score(gt_n,pred_n))/2)
	# print "\t\t[Relevant]\t - recall: %f - precision: %f" % (recall_pos, precision_score(gt,pred))
	# print "\t\t[Irrelevant]\t - recall: %f - precision: %f" % (recall_neg, precision_score(gt_n,pred_n))

word2VecPath = '/sb-personal/cvqa/data/word2vec/google-news/GoogleNews-vectors-negative300.bin'
captionFile = '/sb-personal/cvqa/data/cvqa/imagecaptions.json'
questionsFile = '/sb-personal/cvqa/data/cvqa/cvqa-sameQuestionDataset-subset-list2.json'

maxQuestionLength = 8
maxCaptionLength = 16
wordVectorSize = 300
embeddingSize = 250
numberOfEpochs = 40
subsetCount = 4000
excludeWordList = ['is','a','the','what','that','to','who','why']

print "Loading Word2Vec Dictionary. This may take a long time..."
w2v = models.Word2Vec.load_word2vec_format(word2VecPath, binary=True)

print "Loading Captions generated by a Pre-Trained Captioning Model for Images..."
imageCaptions = readJson(captionFile)

print "Loading Questions..."
questionRows = readJson(questionsFile)
questionKeys = questionRows.keys()[:subsetCount]

for i,questionRowIndex in enumerate(questionKeys):
	questionRow = questionRows[questionRowIndex]
	imageFilename = questionRow['image']
	caption = imageCaptions[imageFilename]

	questionWords = questionRow['question'].split(' ')
	captionWords = caption.split(' ')

	maxQuestionLength = max(maxQuestionLength, len(questionWords))
	maxCaptionLength = max(maxCaptionLength, len(captionWords))


totalFeatureLength = maxQuestionLength + maxCaptionLength

print 'Total Question Rows: [%d]' % (len(questionRows))
print 'Filtered Question Rows: [%d]' % (len(questionKeys))

X = []
y = []

maxLength = max(maxQuestionLength, maxCaptionLength)
totalLength = maxQuestionLength + maxCaptionLength

print 'Max Sequence Length: [%d]' % (maxLength)
print 'Total Sequence Length: [%d]' % (totalLength)

for i,questionRowIndex in enumerate(questionKeys):
	questionRow = questionRows[questionRowIndex]
	imageFilename = questionRow['image']
	caption = imageCaptions[imageFilename]

	labels = [int(l) for l in questionRow['wordLabels'].split(' ')]
	questionWords = questionRow['question'].split(' ')
	captionWords = caption.split(' ')

	newLabels = []
	newQuestionWords = []
	newCaptionWords = []
	for wi, w in enumerate(questionWords):
		if w in w2v:
			if w.lower() not in excludeWordList:
				newQuestionWords.append(w)
				newLabels.append(labels[wi])
	for w in captionWords:
		if w in w2v:
			if w.lower() not in excludeWordList:
				newCaptionWords.append(w)

	labels = newLabels
	questionWords = newQuestionWords
	captionWords = newCaptionWords

	captionFeature = np.zeros((maxCaptionLength, wordVectorSize))
	questionFeature = np.zeros((maxQuestionLength, wordVectorSize))
	labelFeature = np.ones((totalLength,1))

	for li,l in enumerate(labels):
		labelFeature[maxCaptionLength+li][0] = l

	for ci,c in enumerate(captionWords):
		captionFeature[ci] = w2v[c]

	for ci,c in enumerate(questionWords):
		questionFeature[ci] = w2v[c]

	X.append(np.append(captionFeature,questionFeature, axis=0))
	# X_cap.append(captionFeature)
	# X_ques.append(questionFeature)
	y.append(labelFeature)

cutoff = len(y)/2
# X_cap_train = np.asarray(X_cap[:cutoff])
# X_cap_test = np.asarray(X_cap[cutoff:])

# X_ques_train = np.asarray(X_ques[:cutoff])
# X_ques_test = np.asarray(X_ques[cutoff:])

y_train = np.asarray(y[:cutoff])
y_test = np.asarray(y[cutoff:])

X_train = np.asarray(X[:cutoff])
X_test = np.asarray(X[cutoff:])

print 'Total data samples: [%d]' % (len(y))
print '\tTraining data size: [%d]' % (len(y_train))
print '\tTest data size: [%d]' % (len(y_test))

n_hidden = 40

decoder = Sequential()
decoder.add(GRU(n_hidden, return_sequences=True, input_shape=(totalLength, wordVectorSize)))
decoder.add(TimeDistributed(Dense(n_hidden/2)))
decoder.add(TimeDistributed(Dense(1, activation='sigmoid')))
decoder.compile(loss='mse', optimizer='rmsprop', metrics=['accuracy', 'precision','recall'])

print(decoder.summary())
print(decoder.get_config())

class TestNormalizedLoss(keras.callbacks.Callback):

	def on_epoch_end(self, epoch, logs={}):
		print ''
		score(y_train, self.model.predict_proba(X_train, verbose=0), "Train")
		score(y_test, self.model.predict_proba(X_test, verbose=0), "Test")
normalizedLoss = TestNormalizedLoss()
# decoder.fit(X_train,y_train, nb_epoch=numberOfEpochs, verbose=1, callbacks=[normalizedLoss])
decoder.fit(X_train,y_train, nb_epoch=numberOfEpochs, verbose=1)
y_predict = decoder.predict_proba(X_test, verbose=0)
# print 'Before'
# print y_predict
for value in np.nditer(y_predict, op_flags=['readwrite']):
	if value > 0.15:
		value[...] = 1
	else:
		value[...] = 0
# print 'After'
# print y_predict

y_predict = np.reshape(y_predict,(len(y_test),totalLength))
y_test = np.reshape(y_test,(len(y_test),totalLength))
print 'Recall: [%f]' % (recall_score(y_test,y_predict))
print 'Precision: [%f]' % (precision_score(y_test,y_predict))
print 'Accuracy: [%f]' % (accuracy_score(y_test,y_predict))

totals = []
for p,_ in enumerate(y_predict):
	totals.append({'predict':np.array_str(y_predict[p]), 'actual':np.array_str(y_test[p])})

outputFile = '/sb-personal/cvqa/data/cvqa/output.csv'
pd.DataFrame(totals).to_csv(outputFile)