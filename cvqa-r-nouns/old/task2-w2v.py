import os
from gensim import models
import json
import numpy as np
from keras.models import *
from keras.layers import *
import keras
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
import keras.backend as K

def readJson(filename):
	print "Reading [%s]..." % (filename)
	with open(filename) as inputFile:
		jsonData = json.load(inputFile)
	print "Finished reading [%s]." % (filename)
	return jsonData

def score(groundTruth, prediction, label):
	thresh=0.15
	pred=prediction>thresh
	gt=np.asarray(groundTruth)>0.5
	pred_n=prediction<thresh
	gt_n=np.asarray(groundTruth)<0.5

	recall_pos = recall_score(gt,pred)
	recall_neg = recall_score(gt_n,pred_n)
	print "\t%s - normalized accuracy: \t%f " % (label, (recall_score(gt,pred)+recall_score(gt_n,pred_n))/2)
	# print "\t\t[Relevant]\t - recall: %f - precision: %f" % (recall_pos, precision_score(gt,pred))
	# print "\t\t[Irrelevant]\t - recall: %f - precision: %f" % (recall_neg, precision_score(gt_n,pred_n))

word2VecPath = '/sb-personal/cvqa/data/word2vec/google-news/GoogleNews-vectors-negative300.bin'
captionFile = '/sb-personal/cvqa/data/cvqa/imagecaptions.json'
questionsFile = '/sb-personal/cvqa/data/cvqa/cvqa-sameQuestionDataset-subset-list2.json'

maxQuestionLength = 8
maxCaptionLength = 16
wordVectorSize = 300
embeddingSize = 250
numberOfEpochs = 40
subsetCount = 4000
excludeWordList = ['is','a','the','what','that','to','who','why']

print "Loading Word2Vec Dictionary. This may take a long time..."
w2v = models.Word2Vec.load_word2vec_format(word2VecPath, binary=True)

print "Loading Captions generated by a Pre-Trained Captioning Model for Images..."
imageCaptions = readJson(captionFile)

print "Loading Questions..."
questionRows = readJson(questionsFile)

questionKeys = questionRows.keys()

words = {}

for i,questionRowIndex in enumerate(questionKeys):
	questionRow = questionRows[questionRowIndex]
	imageFilename = questionRow['image']
	caption = imageCaptions[imageFilename]

	questionWords = questionRow['question'].split(' ')
	captionWords = caption.split(' ')

	for w in questionWords:
		words[w] = 1
	
	for w in captionWords:
		words[w] = 1

	maxQuestionLength = max(maxQuestionLength, len(questionWords))
	maxCaptionLength = max(maxCaptionLength, len(captionWords))

words = words.keys()

vocabSize = len(words) + 1
totalFeatureLength = maxQuestionLength + maxCaptionLength

print 'Total Question Rows: [%d]' % (len(questionRows))
print len(questionKeys)
print maxQuestionLength
print maxCaptionLength
print vocabSize

word_index = {}
index_word = {}

for i, word in enumerate(words):
    word_index[word] = i + 1
    index_word[i + 1] = word

X_cap = []
X_ques = []
y = []

maxLength = max(maxQuestionLength, maxCaptionLength)

for i,questionRowIndex in enumerate(questionKeys):
	questionRow = questionRows[questionRowIndex]
	imageFilename = questionRow['image']
	caption = imageCaptions[imageFilename]

	labels = [int(l) for l in questionRow['wordLabels'].split(' ')]
	questionWords = questionRow['question'].split(' ')
	captionWords = caption.split(' ')

	newLabels = []
	newQuestionWords = []
	newCaptionWords = []
	for wi, w in enumerate(questionWords):
		if w in w2v:
			if w.lower() not in excludeWordList:
				newQuestionWords.append(w)
				newLabels.append(labels[wi])
	for w in captionWords:
		if w in w2v:
			if w.lower() not in excludeWordList:
				newCaptionWords.append(w)

	labels = newLabels
	questionWords = newQuestionWords
	captionWords = newCaptionWords

	# print captionWords
	# print questionWords
	# print labels

	for currentWi in range(len(questionWords)):
		captionFeature = np.zeros((maxLength, wordVectorSize))
		questionFeature = np.zeros((maxLength, wordVectorSize))

		for ci,c in enumerate(captionWords):
			captionFeature[ci] = w2v[c]

		for wi in range(0,currentWi+1):
			c = questionWords[wi]
			questionFeature[wi] = w2v[c]

		X_cap.append(captionFeature)
		X_ques.append(questionFeature)
		y.append(labels[currentWi])

cutoff = len(y)/2
X_cap_train = np.asarray(X_cap[:cutoff])
X_cap_test = np.asarray(X_cap[cutoff:])

X_ques_train = np.asarray(X_ques[:cutoff])
X_ques_test = np.asarray(X_ques[cutoff:])

y_train = np.asarray(y[:cutoff])
y_test = np.asarray(y[cutoff:])

print len(y)
print len(y_train)

encoder_a = Sequential()
encoder_a.add(LSTM(40, input_shape=(maxLength,wordVectorSize)))

encoder_b = Sequential()
encoder_b.add(LSTM(40, input_shape=(maxLength,wordVectorSize)))

decoder = Sequential()
decoder.add(Merge([encoder_a, encoder_b], mode='concat'))
decoder.add(Dense(40, activation='relu'))
decoder.add(Dense(20, activation='relu'))
decoder.add(Dense(1, activation='sigmoid'))
decoder.compile(loss='binary_crossentropy', optimizer='rmsprop')

print(decoder.summary())
print(decoder.get_config())

class TestNormalizedLoss(keras.callbacks.Callback):

	def on_epoch_end(self, epoch, logs={}):
		print ''
		score(y_train, self.model.predict_proba([X_ques_train,X_cap_train], verbose=0), "Train")
		score(y_test, self.model.predict_proba([X_ques_test,X_cap_test], verbose=0), "Test")
normalizedLoss = TestNormalizedLoss()
decoder.fit([X_ques_train,X_cap_train],y_train, nb_epoch=numberOfEpochs, verbose=1, callbacks=[normalizedLoss])
