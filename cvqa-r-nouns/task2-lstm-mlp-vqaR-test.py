import os
from gensim import models
import json
import numpy as np
from keras.models import *
from keras.layers import *
import keras
from sklearn.metrics import *
import keras.backend as K
import pandas as pd
import argparse
from nltk.tag import pos_tag
import nltk
import sys

reload(sys)  
sys.setdefaultencoding('utf8')

pd.options.mode.chained_assignment = None

def stat(output):
	sys.stdout.write("\r" + output)
	sys.stdout.flush()

def readJson(filename):
	print "Reading [%s]..." % (filename)
	with open(filename) as inputFile:
		jsonData = json.load(inputFile)
	print "Finished reading [%s]." % (filename)
	return jsonData

def filterWords(questionRow, caption):
	questionWords = questionRow['question'].split(' ')
	captionWords = caption.split(' ')

	newQuestionWords = []
	newCaptionWords = []
	for wi, w in enumerate(questionWords):
		if w in w2v:
			if w.lower() not in excludeWordList:
				newQuestionWords.append(w)
	for w in captionWords:
		if w in w2v:
			if w.lower() not in excludeWordList:
				newCaptionWords.append(w)

	return newQuestionWords, newCaptionWords


def extractFeatures(questionRows, totalLength, maxLength):

	# print '\tTotal Question Rows: [%d]' % (len(questionRows))

	X_captions = []
	x_questions = []
	y = []

	for i,questionRow in questionRows.iterrows():
		imageFilename = questionRow['image']
		caption = imageCaptions[imageFilename]

		questionWords, captionWords = filterWords(questionRow, caption)

		captionFeature = np.zeros((maxLength, wordVectorSize))
		questionFeature = np.zeros((maxLength, wordVectorSize))
		
		for ci,c in enumerate(captionWords):
			captionFeature[ci] = w2v[c]

		for ci,c in enumerate(questionWords):
			questionFeature[ci] = w2v[c]

		X_captions.append(captionFeature)
		x_questions.append(questionFeature)

	return np.asarray(x_questions),np.asarray(X_captions)


parser = argparse.ArgumentParser()
parser.add_argument('-d', action='store', dest='dataFile', help='Data file')
parser.add_argument('-m', action='store', dest='modelWeights', help='Model file')
parser.add_argument('-b', action='store', dest='baseDataDirectory', help='Base directory')

results = parser.parse_args()

dataFile = results.dataFile
baseDataDirectory = results.baseDataDirectory
modelWeights = results.modelWeights
outputPath = '/sb-personal/cvqa/results/vqar'

word2VecPath = os.path.join(baseDataDirectory, 'word2vec/google-news/GoogleNews-vectors-negative300.bin')
captionFile = os.path.join(baseDataDirectory, 'vqar/imagecaptions-new.json')

wordVectorSize = 300
maxLength = 20
totalLength = maxLength * 2
excludeWordList = ['is','a','the','what','that','to','who','why']
folds = 40
modelType = 'all-bidirect'

print "Loading Word2Vec Dictionary. This may take a long time..."
w2v = models.Word2Vec.load_word2vec_format(word2VecPath, binary=True)

print "Loading Captions generated by a Pre-Trained Captioning Model for Images..."
imageCaptions = readJson(captionFile)

print "Loading Questions..."

allRows = pd.read_csv(dataFile)

print '\tAll rows: [%d]' % (len(allRows))

print "Removing Questions Without Matching Captions..."

allRows = allRows[allRows['image'].isin(imageCaptions)]

print '\tAll rows: [%d]' % (len(allRows))

vocabSize = 3059

print 'Vocab size: [%d]' % (vocabSize)
print 'Max Sequence Length: [%d]' % (maxLength)
print 'Total Sequence Length: [%d]' % (totalLength)

def loadModel():
	# print "Loading model..."

	metrics = ['accuracy', 'precision','recall','fmeasure']

	encoder_a = Sequential()
	encoder_a.add(Bidirectional(LSTM(200), input_shape=(maxLength,wordVectorSize)))

	encoder_b = Sequential()
	encoder_b.add(Bidirectional(LSTM(200), input_shape=(maxLength,wordVectorSize)))

	decoder = Sequential()
	decoder.add(Merge([encoder_a, encoder_b], mode='concat'))
	decoder.add(Dense(150, activation='relu'))
	decoder.add(Dense(100, activation='relu'))
	decoder.add(Dense(vocabSize, activation='softmax'))
	decoder.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=metrics)
	decoder.load_weights(modelWeights)
	return decoder


print 'All rows: [%d]' % (len(allRows))

model = loadModel()

for fold in range(0,folds):

	print "Running fold: [%d] model: [%s]" % (fold, modelType)
	allRows = allRows.sample(frac=1)
	split = len(allRows) * 2/3
	testRows = allRows[split+1:]
	outputResultsFile = os.path.join(outputPath, "%s-%d-outputTestResults.csv" % (modelType, fold))
	X_questions_test, X_captions_test = extractFeatures(testRows, totalLength, maxLength)
	y_predict = model.predict_proba([X_questions_test, X_captions_test], verbose=0)

	y_predict_words = []
	test_captions = []
	index = 0
	for _,t in testRows.iterrows():
		best = np.argmax(y_predict[index])
		imageFilename = t['image']
		test_captions.append(imageCaptions[imageFilename])
		if best == 0:
			y_predict_words.append("RELEVANT")
		else:
			y_predict_words.append("IRRELEVANT")
		index+=1

	testRows['caption'] = pd.Series(test_captions, index=testRows.index)
	testRows['predict'] = pd.Series(y_predict_words, index=testRows.index)
	testRows.to_csv(outputResultsFile)

	print ''
