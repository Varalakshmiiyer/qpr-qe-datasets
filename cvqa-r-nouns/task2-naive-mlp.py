import os
from gensim import models
import json
import numpy as np
from keras.models import *
from keras.layers import *
import keras
from sklearn.metrics import *
import keras.backend as K
import pandas as pd
from collections import OrderedDict

def readJson(filename):
	print "Reading [%s]..." % (filename)
	with open(filename) as inputFile:
		jsonData = json.load(inputFile)
	print "Finished reading [%s]." % (filename)
	return jsonData

def extractFeatures(questionRows, word_index):

	print 'Total Question Rows: [%d]' % (len(questionRows))

	X = []
	y = []

	for i,questionRow in questionRows.iterrows():
		imageFilename = questionRow['image']
		caption = imageCaptions[imageFilename]

		labels = [int(l) for l in questionRow['wordLabels'].split(' ')]
		questionWords = questionRow['question'].split(' ')
		captionWords = caption.split(' ')

		newLabels = []
		newQuestionWords = []
		newCaptionWords = []
		for wi, w in enumerate(questionWords):
			if w in w2v:
				if w.lower() not in excludeWordList:
					newQuestionWords.append(w)
					newLabels.append(labels[wi])
		for w in captionWords:
			if w in w2v:
				if w.lower() not in excludeWordList:
					newCaptionWords.append(w)

		labels = newLabels
		questionWords = newQuestionWords
		captionWords = newCaptionWords

		feature = np.zeros(len(word_index))
		labelFeature = np.zeros(len(word_index))

		relevant = True
		for li,l in enumerate(labels):
			if (l == 0):
				labelFeature[word_index[questionWords[li]]] = 1
				relevant = False

		if relevant:
			labelFeature[0] = 1
		
		for ci,c in enumerate(captionWords):
			feature[word_index[c]] += 1

		for ci,c in enumerate(questionWords):
			feature[word_index[c]] += 1

		X.append(feature)
		y.append(labelFeature)

	return np.asarray(X),np.asarray(y)

def extractVocab(rowSet):
	word_index = {'RELEVANT':0}
	index_word = {0:'RELEVANT'}
	for r in rowSet:
		for i,questionRow in r.iterrows():
			imageFilename = questionRow['image']
			caption = imageCaptions[imageFilename]

			questionWords = questionRow['question'].split(' ')
			for w in questionWords:
				if (w in w2v) and (w not in word_index) and (w not in excludeWordList):
					word_index[w] = len(word_index)
					index_word[word_index[w]] = w
			captionWords = caption.split(' ')
			for w in captionWords:
				if (w in w2v) and (w not in word_index) and (w not in excludeWordList):
					word_index[w] = len(word_index)
					index_word[word_index[w]] = w

	return word_index, index_word


word2VecPath = '/sb-personal/cvqa/data/word2vec/google-news/GoogleNews-vectors-negative300.bin'
captionFile = '/sb-personal/cvqa/data/cvqa/imagecaptions.json'
trainFile = '/sb-personal/cvqa/data/cvqa/cvqa-sameQuestionDataset-list5-test.csv'
testFile = '/sb-personal/cvqa/data/cvqa/cvqa-sameQuestionDataset-list5-train.csv'

maxQuestionLength = 8
maxCaptionLength = 16
wordVectorSize = 300
embeddingSize = 250
numberOfEpochs = 70
n_hidden = 40
excludeWordList = ['is','a','the','what','that','to','who','why']

print "Loading Word2Vec Dictionary. This may take a long time..."
w2v = models.Word2Vec.load_word2vec_format(word2VecPath, binary=True)

print "Loading Captions generated by a Pre-Trained Captioning Model for Images..."
imageCaptions = readJson(captionFile)

print "Loading Questions..."

trainRows = pd.read_csv(trainFile)
testRows = pd.read_csv(testFile)

print "Extracting Vocab..."

word_index, index_word = extractVocab([trainRows, testRows])

print 'Vocab size: [%d]' % (len(word_index))

print 'Extraction Training Features...'
X_train, y_train = extractFeatures(trainRows, word_index)
print 'Extraction Test Features...'
X_test, y_test = extractFeatures(testRows, word_index)

print 'Total data samples: [%d]' % (len(y_train) + len(y_test))
print '\tTraining data size: [%d]' % (len(y_train))
print '\tTest data size: [%d]' % (len(y_test))

decoder = Sequential()
decoder.add(Dense(40, input_dim=len(word_index), activation='relu'))
decoder.add(Dense(15, activation='relu'))
decoder.add(Dense(len(word_index), activation='softmax'))
decoder.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy', 'precision','recall','fmeasure'])

print(decoder.summary())
print(decoder.get_config())

def test():
	names = ['loss','acc', 'precision', 'recall', 'fmeasure']
	scores = decoder.test_on_batch(X_test, y_test)
	totalScores = dict(zip(decoder.metrics_names, scores))
	print 'Test: ' + ' - '.join([n + ": " + str(float(totalScores[n])) for n in names])

class TestCallback(keras.callbacks.Callback):

	def on_epoch_end(self, epoch, logs={}):
		# print ''
		test()
testCallback = TestCallback()
decoder.fit(X_train, y_train, nb_epoch=numberOfEpochs, verbose=1, callbacks=[testCallback])

test()
